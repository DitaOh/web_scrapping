# Web Scrapping Project - Political Data Collection

**Deadline:** September 24, 11:59 PM  
**Google Sheet:** [Link to submissions](https://docs.google.com/spreadsheets/d/10vy5pyNvLUAb2pCJLye7gaOPHOIhjdmzUTKdiUbF5oI/edit?usp=sharing)

## Project Structure
```
web_scrapping/
â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ api_reddit.py           # âœ… COMPLETED - Exercise 1
â”‚   â””â”€â”€ web_scrapping_bumeran.py # ðŸ”„ TO DO - Exercise 2
â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ reddit_posts_*.csv      # âœ… Generated by Exercise 1
â”‚   â”œâ”€â”€ reddit_comments_*.csv   # âœ… Generated by Exercise 1
â”‚   â””â”€â”€ bumeran_jobs.csv        # ðŸ”„ TO DO - Exercise 2 output
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## Exercise 1: Reddit API Data Collection âœ… COMPLETED

**Completed by:** [Your name/username]  
**Status:** Finished  
**Files generated:**
- `code/api_reddit.py`
- `output/reddit_posts_YYYYMMDD_HHMMSS.csv`
- `output/reddit_comments_YYYYMMDD_HHMMSS.csv`

### What was implemented:
- Reddit API connection using PRAW
- Data collection from r/politics, r/PoliticalDiscussion, r/worldnews
- 20 posts per subreddit with metadata (title, score, num_comments, id, url)
- 5 comments per post with metadata (body, score, post_id)
- Environment variables for secure credential management
- CSV export functionality

## Exercise 2: Bumeran Web Scraping ðŸ”„ TO DO

**Target:** Data Science job offers from Bumeran platform  
**Status:** Available for collaboration  

### Requirements:
1. **Setup environment:**
   ```bash
   pip install requests beautifulsoup4 selenium pandas
   ```

2. **Target URL with filters:**
   - Platform: Bumeran
   - Job category: Data Science
   - Apply all specified filters from instructions

3. **Two-stage scraping strategy:**
   - **Stage 1:** Extract all job posting URLs
   - **Stage 2:** Scrape detailed information for each job

4. **Required data extraction:**
   - Job Title
   - Description (up to "Benefits" section)
   - District
   - Work Mode (on-site, remote, hybrid)

5. **Output:** `output/bumeran_jobs.csv`

6. **File to create:** `code/web_scrapping_bumeran.py`

## Getting Started

### For Exercise 1 (Reddit API):
The Reddit scraping is already complete. To run it:

1. Create a `.env` file in the `code/` directory:
```env
REDDIT_CLIENT_ID=your_client_id_here
REDDIT_CLIENT_SECRET=your_client_secret_here
REDDIT_USERNAME=your_username_here
REDDIT_PASSWORD=your_password_here
```

2. Run the script:
```bash
cd code/
python api_reddit.py
```

### For Exercise 2 (Bumeran Scraping):
1. Clone this repository
2. Install requirements: `pip install -r requirements.txt`
3. Create `code/web_scrapping_bumeran.py`
4. Implement the scraping logic following the two-stage strategy
5. Generate `output/bumeran_jobs.csv`

## Collaboration Instructions

### If you're working on Exercise 2:
1. **Clone the repo:** `git clone [repository-url]`
2. **Create a new branch:** `git checkout -b exercise-2-[yourname]`
3. **Work on:** `code/web_scrapping_bumeran.py`
4. **Test your code** and generate the CSV output
5. **Create a Pull Request** when ready

### Video Requirements (3 minutes):
- Environment setup explanation
- Code walkthrough
- Sample run demonstration
- Upload to YouTube (public/unlisted)
- Submit link in the Google Sheet

## Important Notes
- **Never commit `.env` files** - they contain sensitive credentials
- **Test thoroughly** before submitting
- **Follow the exact output format** specified in instructions
- **Deadline is strict:** September 24, 11:59 PM

## Contact
If you have questions about Exercise 1 implementation or need clarification, reach out to [your contact info].

---
**Status Updated:** [Current date]