# API Political Data Collection - Web Scrapping Project

**Deadline:** September 24, 11:59 PM  
**Google Sheet:** [Link to submissions](https://docs.google.com/spreadsheets/d/10vy5pyNvLUAb2pCJLye7gaOPHOIhjdmzUTKdiUbF5oI/edit?usp=sharing)

## Project Structure
```
web_scrapping/
├── code/
│   ├── api_reddit.py               # ✅ COMPLETED - Exercise 1
│   └── web_scrapping_bumeran.ipynb # ✅ COMPLETED - Exercise 2
├── output/
│   ├── reddit_posts_*.csv      # ✅ Generated by Exercise 1
│   ├── reddit_comments_*.csv   # ✅ Generated by Exercise 1
│   └── bumeran_jobs.csv        # ✅ Generated by Exercise 2
├── requirements.txt
└── README.md
```

## Exercise 1: Reddit API Data Collection ✅ COMPLETED

**Completed by:** 
- [Diana/@Diane-12]
- [Pedro Villena/@pedrovillena]
- [María Paula Dávila/@mdaviladv1]
- [Nadia Duffó/@naddc]

**Status:** Finished  
**Files generated:**
- `code/api_reddit.py`
- `output/reddit_posts_YYYYMMDD_HHMMSS.csv`
- `output/reddit_comments_YYYYMMDD_HHMMSS.csv`

### What was implemented:
- Reddit API connection using PRAW
- Data collection from r/politics, r/PoliticalDiscussion, r/worldnews
- 20 posts per subreddit with metadata (title, score, num_comments, id, url)
- 5 comments per post with metadata (body, score, post_id)
- Environment variables for secure credential management
- CSV export functionality

## Exercise 2: Bumeran Web Scraping ✅ COMPLETED

**Target:** Data Science job offers from Bumeran platform  
**Completed by:** [Diana/@Diane-12] [Pedro Villena/@pedrovillena] [María Paula Dávila/@mdaviladv1] [Nadia Duffó/@naddc]
**Status:** Finished
**Files generated:**
- web_scrapping_bumeran.ipynb
- bumeran_jobs.csv

### Requirements:
1. **Setup environment:**
   ```bash
   pip install selenium pandas re time numpy webdriver-manager
   ```

2. **Target URL with filters:**
   - Platform: Bumeran
   - Job category: Data Science
   - Apply all specified filters from instructions

3. **Two-stage scraping strategy:**
   - **Stage 1:** Extract all job posting URLs
   - **Stage 2:** Scrape detailed information for each job

4. **Required data extraction:**
   - Job Title
   - Description (up to "Benefits" section)
   - District
   - Work Mode (on-site, remote, hybrid)

5. **Output:** `output/bumeran_jobs.csv`

6. **File to create:** `code/web_scrapping_bumeran.py`

## Getting Started

### For Exercise 1 (Reddit API):
The Reddit scraping is already complete. To run it:

1. Create a `.env` file in the `code/` directory:
```env
REDDIT_CLIENT_ID=your_client_id_here
REDDIT_CLIENT_SECRET=your_client_secret_here
REDDIT_USERNAME=your_username_here
REDDIT_PASSWORD=your_password_here
```

2. Run the script:
```bash
cd code/
python api_reddit.py
```

### For Exercise 2 (Bumeran Scraping):
1. Clone this repository
2. Install requirements: `pip install -r requirements.txt`
3. Create `code/web_scrapping_bumeran.py`
4. Implement the scraping logic following the two-stage strategy
5. Generate `output/bumeran_jobs.csv`

### Video Requirements (3 minutes):
- Environment setup explanation
- Code walkthrough
- Sample run demonstration
- Upload to YouTube (public/unlisted)
- Submit link in the Google Sheet

## Important Notes
- **Never commit `.env` files** - they contain sensitive credentials (include in .gitignore)
- **Test thoroughly** before submitting
- **Follow the exact output format** specified in instructions
- **Deadline is strict:** September 24, 11:59 PM

---
**Status Updated:** [Current date]
