{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60894044",
   "metadata": {},
   "source": [
    "# Exercise 2: Bumeran — Web Scrapping\n",
    "\n",
    "## 1. Setup\n",
    "\n",
    "We already have created an env and installed requirements.txt\n",
    "\n",
    "Now we call libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d85bd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import re\n",
    "import time \n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abcbdaf",
   "metadata": {},
   "source": [
    "## 2. Web Scrapping\n",
    "### Stage 1: Extract Job Posting Links\n",
    "\n",
    "We'll scrape all the job listing URLs based on the following filters:\n",
    "  - Menor a 15 días\n",
    "  - Tecnologías, Sistemas y Telecomunicaciones\n",
    "  - Programación\n",
    "  - Lima\n",
    "  - Full-time\n",
    "\n",
    "We start by launching the driver:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9571fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Launching Chrome driver...)')\n",
    "options = Options()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842216e5",
   "metadata": {},
   "source": [
    "And set the driver url:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf163cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Launching Bumeran url...')\n",
    "\n",
    "url = 'https://www.bumeran.com.pe/empleos.html'\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "\n",
    "driver.maximize_window()\n",
    "driver.execute_script(\"document.body.style.zoom='100%'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a0ccb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Current page title: ', driver.title, '\\nCurrent Page URL: ', driver.current_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cba57a",
   "metadata": {},
   "source": [
    "Now, we apply filters by code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1b7ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Time to apply filters...')\n",
    "\n",
    "steps = [\n",
    "    # Fecha de publicación\n",
    "    (\"Open Fecha de publicación\", \"//button[contains(.,'Fecha de publicación')]\"),\n",
    "    (\"Select Menor a 15 días\", \"//button[contains(.,'Menor a 15 días')]\"),\n",
    "\n",
    "    # Área\n",
    "    (\"Open menú Área\", \"//button[contains(.,'Área')]\"),\n",
    "    (\"Select Tecnologías, Sistemas y Telecomunicaciones\", \"//button[contains(.,'Tecnología, Sistemas y Telecomunicaciones')]\"),\n",
    "\n",
    "    # Subárea\n",
    "    (\"Open Subárea\", \"//button[contains(.,'Subárea')]\"),\n",
    "    (\"Select Programación\", \"//button[contains(.,'Programación')]\"),\n",
    "\n",
    "    # Departamento\n",
    "    (\"Open Departamento\", \"//button[contains(.,'Departamento')]\"),\n",
    "    (\"Select Lima\", \"//button[contains(.,'Lima')]\"),\n",
    "\n",
    "    # Carga horaria\n",
    "    (\"Open Carga horaria\", \"//button[contains(.,'Carga horaria')]\"),\n",
    "    (\"Select Full-time\", \"//button[contains(.,'Full-time')]\")\n",
    "]\n",
    "\n",
    "for desc, xpath in steps:\n",
    "    print(desc)\n",
    "    btn = driver.find_element(By.XPATH, xpath)\n",
    "    driver.execute_script(\"arguments[0].click();\", btn)\n",
    "    print('Sleep time zzz...')\n",
    "    time.sleep(2)\n",
    "\n",
    "print(\"All Filters applied.\")\n",
    "\n",
    "\n",
    "filtered_url = driver.current_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f52568",
   "metadata": {},
   "source": [
    "After that, we can scrape the page to get all urls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b899d0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_job_links = []\n",
    "unique_job_links = []\n",
    "base_url = \"https://www.bumeran.com.pe\"\n",
    "page = 1\n",
    "\n",
    "driver.get(filtered_url)\n",
    "time.sleep(3)\n",
    "\n",
    "print('Stage 1 scrapping starts!')\n",
    "while True:\n",
    "    job_posts = driver.find_elements(\n",
    "        By.XPATH,\n",
    "        \"//div[@id='listado-avisos']//a[contains(@href,'/empleos/')]\"\n",
    "    )\n",
    "    for post in job_posts:\n",
    "        href = post.get_attribute(\"href\")\n",
    "        if href.startswith(\"/\"):\n",
    "            href = base_url + href\n",
    "\n",
    "        # guardar todos\n",
    "        all_job_links.append(href)\n",
    "\n",
    "        # guardar únicos\n",
    "        if href not in unique_job_links:\n",
    "            unique_job_links.append(href)\n",
    "\n",
    "    print(f\"Page {page} scraped. Total so far: {len(all_job_links)} (all), {len(unique_job_links)} (unique)\")\n",
    "\n",
    "    # pasar a la siguiente página\n",
    "    page += 1\n",
    "    try:\n",
    "        next_page = driver.find_element(By.XPATH, f\"//a[span[text()='{page}']]\")\n",
    "        driver.execute_script(\"arguments[0].click();\", next_page)\n",
    "        print('Sleep time zzz...')\n",
    "        time.sleep(5)\n",
    "    except Exception:\n",
    "        print(\"Last page reached.\")\n",
    "        break\n",
    "\n",
    "print(f\"Total links (with duplicates): {len(all_job_links)}\")\n",
    "print(f\"Total unique links: {len(unique_job_links)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c0cae9",
   "metadata": {},
   "source": [
    "### Stage 2: Scrape Job Details\n",
    "\n",
    "For each job URL collected in Stage 1, extract the following:\n",
    "  - Job Title\n",
    "  - Description (up to the \"Benefits\" section)\n",
    "  - District\n",
    "  - Work Mode (e.g., on-site, remote, hybrid)\n",
    "\n",
    "We start by launching the driver:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8331d8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Stage 2 (robust): extract Job Title, Description, District, Work Mode =====\n",
    "# Reuses: driver, unique_job_links from Stage 1\n",
    "\n",
    "import os, re, csv, time, json, html, unicodedata\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
    "\n",
    "output_dir = \"../output\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "OUTPUT_CSV = os.path.join(os.getcwd(), \"../output\", \"output_stage2.csv\")\n",
    "WAIT_ELEM = 9\n",
    "PAGELOAD_TIMEOUT = 7.5\n",
    "SCRIPT_TIMEOUT = 5\n",
    "PAUSE = 0.2\n",
    "\n",
    "DISTRICTS = [\n",
    "    \"Lima\",\"Callao\",\"Miraflores\",\"San Isidro\",\"Surco\",\"La Molina\",\"San Borja\",\n",
    "    \"Barranco\",\"Chorrillos\",\"La Victoria\",\"San Miguel\",\"Pueblo Libre\",\"Magdalena\",\n",
    "    \"Jesús María\",\"Jesus Maria\",\"Magdalena del Mar\",\"Santiago de Surco\",\"Ate\",\"Lurín\",\"Lurin\",\"Cercado de Lima\"\n",
    "]\n",
    "\n",
    "# timeouts (ignore if not supported)\n",
    "try:\n",
    "    driver.set_page_load_timeout(PAGELOAD_TIMEOUT)\n",
    "    driver.set_script_timeout(SCRIPT_TIMEOUT)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "def norm(s: str) -> str:\n",
    "    if not s: return \"\"\n",
    "    return re.sub(r\"\\s+\", \" \", s, flags=re.M).strip()\n",
    "\n",
    "def strip_accents(s: str) -> str:\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def strip_html(s: str) -> str:\n",
    "    if not s: return \"\"\n",
    "    s = html.unescape(s)\n",
    "    s = re.sub(r\"<br\\s*/?>\", \"\\n\", s, flags=re.I)\n",
    "    s = re.sub(r\"</p\\s*>\", \"\\n\", s, flags=re.I)\n",
    "    s = re.sub(r\"<.*?>\", \"\", s)  # remove tags\n",
    "    return norm(s)\n",
    "\n",
    "def safe_get(url: str):\n",
    "    try:\n",
    "        driver.get(url)\n",
    "    except TimeoutException:\n",
    "        try:\n",
    "            driver.execute_script(\"window.stop();\")\n",
    "        except WebDriverException:\n",
    "            pass\n",
    "\n",
    "def wait_job_loaded(timeout=WAIT_ELEM):\n",
    "    xp = (\"//h1 | //script[@type='application/ld+json']\"\n",
    "          \" | //div[contains(@id,'jobDescription') or contains(@class,'job-description') or @data-qa='job-description']\")\n",
    "    WebDriverWait(driver, timeout).until(EC.presence_of_element_located((By.XPATH, xp)))\n",
    "\n",
    "def is_valid_job_url(u: str) -> bool:\n",
    "    return bool(re.search(r\"https?://(www\\.)?bumeran\\.com\\.pe/empleos/.+?-([0-9]{6,})\\.html\", u, re.I))\n",
    "\n",
    "# ---------------- JSON-LD first, DOM second ----------------\n",
    "\n",
    "def parse_jsonld():\n",
    "    \"\"\"Return dict with any of: title, description, district, mode from JobPosting JSON-LD, else {}.\"\"\"\n",
    "    out = {}\n",
    "    scripts = driver.find_elements(By.XPATH, \"//script[@type='application/ld+json']\")\n",
    "    for sc in scripts:\n",
    "        raw = sc.get_attribute(\"textContent\") or \"\"\n",
    "        raw = raw.strip()\n",
    "        if not raw:\n",
    "            continue\n",
    "        # some sites wrap multiple JSON objects or arrays\n",
    "        try:\n",
    "            data = json.loads(raw)\n",
    "        except Exception:\n",
    "            # try to salvage by fixing trailing commas etc. If it fails, skip.\n",
    "            continue\n",
    "\n",
    "        candidates = []\n",
    "        if isinstance(data, list):\n",
    "            candidates = data\n",
    "        else:\n",
    "            candidates = [data]\n",
    "\n",
    "        for obj in candidates:\n",
    "            # Some pages nest under @graph\n",
    "            if isinstance(obj, dict) and \"@graph\" in obj and isinstance(obj[\"@graph\"], list):\n",
    "                candidates.extend(obj[\"@graph\"])\n",
    "                continue\n",
    "\n",
    "            if not isinstance(obj, dict): \n",
    "                continue\n",
    "            t = obj.get(\"@type\") or obj.get(\"type\")\n",
    "            if isinstance(t, list):\n",
    "                t = next((x for x in t if isinstance(x,str) and x.lower()==\"jobposting\"), t[0] if t else None)\n",
    "            if not isinstance(t, str) or \"jobposting\" not in t.lower():\n",
    "                continue\n",
    "\n",
    "            # title\n",
    "            title = obj.get(\"title\") or obj.get(\"name\")\n",
    "            if title: out.setdefault(\"title\", norm(title))\n",
    "\n",
    "            # description (may be HTML)\n",
    "            desc = obj.get(\"description\")\n",
    "            if desc:\n",
    "                desc = strip_html(desc)\n",
    "                # cut at Beneficios with/without accent\n",
    "                lo = strip_accents(desc.lower())\n",
    "                m = re.search(r\"\\bbeneficios?\\b\", lo)\n",
    "                if m: desc = desc[:m.start()].rstrip()\n",
    "                out.setdefault(\"description\", desc[:10000])\n",
    "\n",
    "            # work mode: remote/hybrid/on site\n",
    "            # schema fields may include jobLocationType, employmentType, or \"applicantLocationRequirements\"\n",
    "            jltype = (obj.get(\"jobLocationType\") or \"\")\n",
    "            etype  = obj.get(\"employmentType\")\n",
    "            text_pool = \" \".join([str(jltype), str(etype or \"\")])\n",
    "            if re.search(r\"remote|telecommute|home\", text_pool, re.I):\n",
    "                out.setdefault(\"mode\", \"Remoto\")\n",
    "            # hybrid often not explicit; leave to DOM if not remote/presencial\n",
    "\n",
    "            # district: jobLocation.address.addressLocality or addressRegion\n",
    "            jl = obj.get(\"jobLocation\")\n",
    "            if isinstance(jl, list):\n",
    "                jl = jl[0] if jl else None\n",
    "            if isinstance(jl, dict):\n",
    "                addr = jl.get(\"address\") or {}\n",
    "                locality = addr.get(\"addressLocality\") or addr.get(\"addressRegion\") or addr.get(\"streetAddress\")\n",
    "                if locality:\n",
    "                    out.setdefault(\"district\", norm(locality))\n",
    "\n",
    "            # If we already have description and district, good enough\n",
    "    return out\n",
    "\n",
    "def dom_title():\n",
    "    for xp in [\"//h1\", \"//header//h1\", \"//h1[contains(@class,'title') or contains(@class,'job')]\"]:\n",
    "        try:\n",
    "            el = driver.find_element(By.XPATH, xp)\n",
    "            t = norm(el.text)\n",
    "            if t: return t\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "def dom_description():\n",
    "    for xp in [\n",
    "        \"//section[contains(@class,'job-description')]\",\n",
    "        \"//div[@data-qa='job-description']\",\n",
    "        \"//div[contains(@id,'jobDescription') or contains(@class,'job-description')]\",\n",
    "        \"//article[contains(@class,'job-description')]\",\n",
    "        # broader fallbacks\n",
    "        \"//main//section[contains(.,'Requis') or contains(.,'Respons')]\",\n",
    "        \"//main//div[contains(.,'Requis') or contains(.,'Respons')]\",\n",
    "        \"//article\"\n",
    "    ]:\n",
    "        try:\n",
    "            el = driver.find_element(By.XPATH, xp)\n",
    "            txt = norm(el.text)\n",
    "            if not txt: \n",
    "                continue\n",
    "            lo = strip_accents(txt.lower())\n",
    "            m = re.search(r\"\\bbeneficios?\\b\", lo)\n",
    "            if m: txt = txt[:m.start()].rstrip()\n",
    "            return txt[:10000]\n",
    "        except Exception:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def dom_blocks():\n",
    "    blocks = []\n",
    "    for xp in [\n",
    "        \"//*[contains(.,'Lugar de trabajo') or contains(.,'Ubicación')]/..\",\n",
    "        \"//li[contains(.,'Lugar de trabajo') or contains(.,'Ubicación')]\",\n",
    "        \"//*[contains(@data-qa,'job-location') or contains(@class,'job-location')]\",\n",
    "        \"//section[contains(@class,'job-details') or contains(@class,'metadata') or contains(@class,'job-info')]\",\n",
    "        \"//dl | //ul\"\n",
    "    ]:\n",
    "        try:\n",
    "            els = driver.find_elements(By.XPATH, xp)\n",
    "            for el in els:\n",
    "                t = norm(el.text)\n",
    "                if t:\n",
    "                    blocks.append(t)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return blocks\n",
    "\n",
    "def dom_location_and_mode():\n",
    "    blocks = dom_blocks()\n",
    "    district, best = None, 10**9\n",
    "    for t in blocks:\n",
    "        for hint in DISTRICTS:\n",
    "            if re.search(rf\"\\b{re.escape(hint)}\\b\", t, re.I):\n",
    "                parts = [p.strip() for p in re.split(r\"[•|,;/\\-]\\s*\", t) if p.strip()]\n",
    "                for p in parts:\n",
    "                    if re.search(rf\"\\b{re.escape(hint)}\\b\", p, re.I) and len(p) < best:\n",
    "                        district, best = p, len(p)\n",
    "\n",
    "    joined = \" | \".join(blocks)\n",
    "    mode = None\n",
    "    if re.search(r\"\\b(remoto|teletrabajo)\\b\", joined, re.I):\n",
    "        mode = \"Remoto\"\n",
    "    elif re.search(r\"\\b(h[ií]brido|mixto|semi ?presencial)\\b\", joined, re.I):\n",
    "        mode = \"Híbrido\"\n",
    "    elif re.search(r\"\\b(presencial|on ?site)\\b\", joined, re.I):\n",
    "        mode = \"Presencial\"\n",
    "\n",
    "    return district, mode\n",
    "\n",
    "def scrape_one(url: str):\n",
    "    safe_get(url)\n",
    "    try:\n",
    "        wait_job_loaded()\n",
    "    except TimeoutException:\n",
    "        return None\n",
    "\n",
    "    data = parse_jsonld()\n",
    "\n",
    "    title = data.get(\"title\") or dom_title()\n",
    "    desc  = data.get(\"description\") or dom_description()\n",
    "    district = data.get(\"district\")\n",
    "    mode = data.get(\"mode\")\n",
    "\n",
    "    if not district or not mode:\n",
    "        d2, m2 = dom_location_and_mode()\n",
    "        district = district or d2\n",
    "        mode = mode or m2\n",
    "\n",
    "    # sanitize district further: keep the shortest district-like token\n",
    "    if district:\n",
    "        tokens = [p.strip() for p in re.split(r\"[•|,;/\\-]\\s*\", district) if p.strip()]\n",
    "        for tk in sorted(tokens, key=len):\n",
    "            for hint in DISTRICTS:\n",
    "                if re.search(rf\"\\b{re.escape(hint)}\\b\", tk, re.I):\n",
    "                    district = tk\n",
    "                    break\n",
    "            if district == tk:\n",
    "                break\n",
    "\n",
    "    return {\n",
    "        \"Job Title\": title or None,\n",
    "        \"Description\": desc or None,\n",
    "        \"District\": district or None,\n",
    "        \"Work Mode\": mode or None,\n",
    "        \"url\": url\n",
    "    }\n",
    "\n",
    "# ---------- run Stage 2 on your Stage-1 links ----------\n",
    "try:\n",
    "    LINKS = list(dict.fromkeys(unique_job_links))\n",
    "except NameError:\n",
    "    LINKS = []\n",
    "\n",
    "VALID = [u for u in LINKS if is_valid_job_url(u)]\n",
    "rows, fails = [], 0\n",
    "\n",
    "for i, url in enumerate(VALID, 1):\n",
    "    print(f\"[{i}/{len(VALID)}] {url}\")\n",
    "    try:\n",
    "        row = scrape_one(url)\n",
    "        if row:\n",
    "            rows.append(row)\n",
    "        else:\n",
    "            fails += 1\n",
    "    except Exception as e:\n",
    "        print(\"skip:\", e)\n",
    "        fails += 1\n",
    "    time.sleep(PAUSE)\n",
    "\n",
    "with open(OUTPUT_CSV, \"w\", encoding=\"utf-8-sig\", newline=\"\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"Job Title\",\"Description\",\"District\",\"Work Mode\",\"url\"])\n",
    "    w.writeheader()\n",
    "    for r in rows:\n",
    "        w.writerow({k: r.get(k) if r.get(k) is not None else \"\" for k in w.fieldnames})\n",
    "\n",
    "print(f\"\\nStage 2 done. Rows OK: {len(rows)} | Failed: {fails} | CSV: {OUTPUT_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d94d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../output/bumeran_jobs.csv\")\n",
    "\n",
    "print(\"Rows:\", len(df))\n",
    "print(\"Missing Description:\", df['Description'].isna().sum())\n",
    "print(\"Missing District:\", df['District'].isna().sum())\n",
    "print(\"Work modes:\", df['Work Mode'].dropna().unique().tolist())\n",
    "\n",
    "print(\"\\nSample:\")\n",
    "display(df.sample(5, random_state=0))\n",
    "\n",
    "# sanity: descriptions shouldn’t contain menu junk\n",
    "bad_desc = df['Description'].fillna('').str.contains(r'Buscar empleo|Trabajos en Lima', case=False)\n",
    "print(\"\\nDescriptions with menu noise:\", bad_desc.sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "selenium_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
